{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the set of commands we want to classify with hmm\n",
    "import librosa\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "commands = [\"demo_fider_ac\", # 0\n",
    "            \"demo_fider_bilgileri\", # 1 \n",
    "            \"demo_fider_kapat\", # 2\n",
    "            \"nem_durumu\", # 3\n",
    "            \"gsm_durumu\" # 4\n",
    "            ]\n",
    "# since we have multiple .wav files in the dirs\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "dataset_path = \"/home/ix502iv/Documents/Audio_Trad/HMM/custom_commands\"\n",
    "for i, command in enumerate(commands):\n",
    "    # loop through each file in the folder\n",
    "    for file in os.listdir(dataset_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_path = os.path.join(dataset_path, file) # there's need to have the full path to the file.\n",
    "            # print(file_path)\n",
    "            audio_data, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "            # print(\"File:\", filename)\n",
    "            # print(\"Sample rate:\", sr)\n",
    "            # print(\"Number of samples:\", len(audio_data))\n",
    "\n",
    "            mfcc = librosa.feature.mfcc(\n",
    "                y=audio_data,\n",
    "                sr=sr,\n",
    "                n_mfcc=13\n",
    "            )\n",
    "\n",
    "            data.append(mfcc.T)\n",
    "            labels.append(i)\n",
    "        \n",
    "# save the features and labels to a .npy file\n",
    "# save an array to a binary file\n",
    "np.save(\"data.npy\", np.vstack(data))\n",
    "np.save(\"labels.npy\", np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 24615 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ix502iv/miniconda3/envs/TensorFlow/lib/python3.10/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=13\n",
      "  return f(*args, **kwargs)\n",
      "/tmp/ipykernel_49170/2568228147.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.save(\"features.npy\", np.array(features))\n"
     ]
    }
   ],
   "source": [
    "# extract the features from each file: mfcc features & save them along with their labels\n",
    "data = np.load(\"data.npy\", allow_pickle=True)\n",
    "data_reshaped = data.ravel()[:100]\n",
    "labels = np.load(\"labels.npy\", allow_pickle=True)\n",
    "print(len(labels), len(data), len(data_reshaped))\n",
    "\n",
    "# param for mfcc extraction\n",
    "n_mfcc = 15\n",
    "\n",
    "# preprocess the .wav file: extract mfcc features and save them with their labe\n",
    "features = []\n",
    "for i in range(len(data_reshaped)): # len(data) = 24615\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y = data[i],\n",
    "        sr = 44100,\n",
    "        n_mfcc= n_mfcc\n",
    "    )\n",
    "\n",
    "    features.append((mfcc.T, labels[i])) \n",
    "    # we run into a size error: IndexError: index 100 is out of bounds for axis 0 with size 100\n",
    "    # labels = 100, 24615 > 100 (actually 99: 0 to 99 = 100): soution reshape data to match 100:\n",
    "    # may reduce the accuracy of our model\n",
    "np.save(\"features.npy\", np.array(features))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and the test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = np.load(\"features.npy\", allow_pickle=True)\n",
    "\n",
    "# the SPLIT\n",
    "X = np.array([f[0] for f in features]) # mfcc features\n",
    "y = np.array([f[1] for f in features]) # labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"y_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdoelling time\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# load the training set and their labels\n",
    "X_train = np.load(\"X_train.npy\", allow_pickle=True)\n",
    "y_train = np.load(\"y_train.npy\", allow_pickle=True)\n",
    "\n",
    "# define the number of states and features for the model\n",
    "n_states = 5 # the number of hidden states\n",
    "n_features = X_train.shape[2] # number of mfcc features : 15 == X_train.shape[2]\n",
    "\n",
    "\n",
    "# initialize the hmm model\n",
    "model = hmm.GaussianHMM(\n",
    "    n_components=n_states,\n",
    "    covariance_type=\"diag\",\n",
    "    n_iter=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training: there is some reshaping of the input features to a 2D array with the shape\n",
    "# (-1, n_features)\n",
    "# model.fit(X_train.reshape(-1, n_features), lengths=[len(seq) for seq in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2D = np.reshape(X_train, (X_train.shape[0],-1))\n",
    "X_test_2D = np.reshape(X_test, (X_test.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianHMM(n_components=5, n_iter=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianHMM</label><div class=\"sk-toggleable__content\"><pre>GaussianHMM(n_components=5, n_iter=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianHMM(n_components=5, n_iter=100)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the testing set of MFCC features and their labels\n",
    "X_test = np.load(\"X_test.npy\", allow_pickle=True)\n",
    "y_test = np.load(\"y_test.npy\", allow_pickle=True)\n",
    "\n",
    "# Define a list of candidate numbers of hidden states to try\n",
    "n_states_list = [3, 5, 7, 9]\n",
    "\n",
    "# Loop over the candidate numbers of hidden states and train a HMM model for each one\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "for n_states in n_states_list:\n",
    "    # Initialize the HMM model\n",
    "    model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"diag\", n_iter=100)\n",
    "\n",
    "    # Train the HMM model using the training set of MFCC features and their labels\n",
    "    # model.fit(X_train.reshape(-1, n_features), lengths=[len(seq) for seq in X_train])\n",
    "    model.fit(X_train_2D)\n",
    "\n",
    "    # Use the HMM model to predict the labels of the testing set of MFCC features\n",
    "    # y_pred = model.predict(X_test.reshape(-1, n_features), lengths=[len(seq) for seq in X_test])\n",
    "    y_pred = model.predict(X_test_2D)\n",
    "\n",
    "    # Compute the accuracy of the HMM model on the testing set of labels\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Update the best model and accuracy if the current model is better than the previous ones\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model\n",
    "\n",
    "# Print the best accuracy and number of hidden states found\n",
    "print(\"Accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the new file to test\n",
    "filename = \"/home/ix502iv/Documents/Audio_Trad/HMM/custom_commands_copy/demo_fider_ac/demo_fider_ac.wav\"\n",
    "y, sr = librosa.load(filename, sr=None)\n",
    "mfcc = librosa.feature.mfcc(\n",
    "    y=y,\n",
    "    sr=sr,\n",
    "    n_mfcc=13,\n",
    "    n_fft=2048,\n",
    "    hop_length=512\n",
    ")\n",
    "\n",
    "# save the mfcc features to a file\n",
    "np.save(\"new_file.npy\", mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use viterbi algorithm to predict the most likely sequence for the features\n",
    "from hmmlearn import hmm\n",
    "# load the trained model\n",
    "model = hmm.GaussianHMM(\n",
    "    n_components=5,\n",
    "    covariance_type=\"diag\",\n",
    "    # n_features=13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.startprob_ = np.array([1.0, 0.0, 0.0, 0.0, 0.0])\n",
    "model.transmat_ = np.array([\n",
    "    [0.5, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.5, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.5, 0.5, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.5, 0.5],\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "])\n",
    "model.means_ = np.random.randn(5, 13)\n",
    "# model.covars_ = np.tile(np.identity(13), (5, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GaussianHMM' object has no attribute '_covars_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m mfcc \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(filename)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Use the Viterbi algorithm to predict the most likely sequence of states\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m logprob, state_sequence \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdecode(mfcc)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLog probability: \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(logprob))\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mState sequence:\u001b[39m\u001b[39m\"\u001b[39m, state_sequence)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hmmlearn/base.py:339\u001b[0m, in \u001b[0;36m_AbstractHMM.decode\u001b[0;34m(self, X, lengths, algorithm)\u001b[0m\n\u001b[1;32m    336\u001b[0m sub_state_sequences \u001b[39m=\u001b[39m []\n\u001b[1;32m    337\u001b[0m \u001b[39mfor\u001b[39;00m sub_X \u001b[39min\u001b[39;00m _utils\u001b[39m.\u001b[39msplit_X_lengths(X, lengths):\n\u001b[1;32m    338\u001b[0m     \u001b[39m# XXX decoder works on a single sample at a time!\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m     sub_log_prob, sub_state_sequence \u001b[39m=\u001b[39m decoder(sub_X)\n\u001b[1;32m    340\u001b[0m     log_prob \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sub_log_prob\n\u001b[1;32m    341\u001b[0m     sub_state_sequences\u001b[39m.\u001b[39mappend(sub_state_sequence)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hmmlearn/base.py:284\u001b[0m, in \u001b[0;36m_AbstractHMM._decode_viterbi\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decode_viterbi\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m--> 284\u001b[0m     log_frameprob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_log_likelihood(X)\n\u001b[1;32m    285\u001b[0m     \u001b[39mreturn\u001b[39;00m _hmmc\u001b[39m.\u001b[39mviterbi(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstartprob_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransmat_, log_frameprob)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hmmlearn/_emissions.py:131\u001b[0m, in \u001b[0;36mBaseGaussianHMM._compute_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_compute_log_likelihood\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m log_multivariate_normal_density(\n\u001b[0;32m--> 131\u001b[0m         X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeans_, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_covars_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcovariance_type)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GaussianHMM' object has no attribute '_covars_'"
     ]
    }
   ],
   "source": [
    "# Load the MFCC features from the new .wav file\n",
    "filename = \"new_file.npy\"\n",
    "mfcc = np.load(filename)\n",
    "\n",
    "# Use the Viterbi algorithm to predict the most likely sequence of states\n",
    "logprob, state_sequence = model.decode(mfcc)\n",
    "print(\"Log probability: {:.2f}\".format(logprob))\n",
    "print(\"State sequence:\", state_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45c48fb264bba0529b917885aa2fdf54bfc5ac58ac8ea30a57d1df6ad7c47fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
